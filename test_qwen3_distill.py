#!/usr/bin/env python3
"""
Qwen3 Knowledge Distillation ν…μ¤νΈ μ¤ν¬λ¦½νΈ
κΈ°λ³Έμ μΈ λ¨λΈ λ΅λ”©κ³Ό νΈν™μ„±μ„ ν™•μΈν•©λ‹λ‹¤.
"""

import torch
import pickle
from utils.prompter import Prompter
from model_qwen3 import LLM4RecQwen3, LLM4RecQwen3Teacher, LLM4RecQwen3Student
from utils.data_utils import LLMDataset, SequentialCollator

def test_qwen3_models():
    """Qwen3 λ¨λΈλ“¤μ κΈ°λ³Έ λ΅λ”©κ³Ό νΈν™μ„± ν…μ¤νΈ"""
    
    print("π” Qwen3 Knowledge Distillation νΈν™μ„± ν…μ¤νΈ μ‹μ‘...")
    
    # κΈ°λ³Έ μ„¤μ •
    base_model = "Qwen/Qwen3-8B"  # μ‹¤μ  ν™κ²½μ—μ„λ” λ΅μ»¬ κ²½λ΅λ΅ λ³€κ²½ κ°€λ¥
    domain_type = "music"
    device_map = {"": 0}
    
    # Prompter μ΄κΈ°ν™”
    prompter = Prompter("alpaca")
    
    try:
        # μ•„μ΄ν… μ„λ² λ”© λ΅λ“ ν…μ¤νΈ
        print("π“¦ μ•„μ΄ν… μ„λ² λ”© λ΅λ“ μ¤‘...")
        item_embed = pickle.load(open(f'./output/{domain_type}.pkl', 'rb'))['item_embedding']
        print(f"β… μ•„μ΄ν… μ„λ² λ”© λ΅λ“ μ„±κ³µ: {item_embed.shape}")
        
        # λ°μ΄ν„°μ…‹ λ΅λ“ ν…μ¤νΈ
        print("π“ λ°μ΄ν„°μ…‹ λ΅λ“ μ¤‘...")
        dataset_train = LLMDataset(
            item_size=999, 
            max_seq_length=30, 
            data_type='train', 
            csv_path=f"./dataset/sequential/{domain_type}.csv"
        )
        print(f"β… ν›λ ¨ λ°μ΄ν„°μ…‹ λ΅λ“ μ„±κ³µ: {len(dataset_train)} μƒν”")
        
        # κΈ°λ³Έ λ¨λΈ μ„¤μ •
        model_args = {
            'base_model': base_model,
            'task_type': 'sequential',
            'cache_dir': './cache',
            'input_dim': 128,
            'output_dim': 0,
            'interval_nums': 0,
            'drop_type': 'trune',
            'lora_r': 16,
            'lora_alpha': 16,
            'lora_dropout': 0.05,
            'lora_target_modules': ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
            'device_map': device_map,
            'instruction_text': prompter.generate_prompt('sequential'),
            'train_stargy': 'lora',
            'user_embeds': None,
            'input_embeds': item_embed,
            'seq_len': 30,
        }
        
        print("π¤– Teacher λ¨λΈ λ΅λ“ ν…μ¤νΈ...")
        teacher_args = model_args.copy()
        teacher_args.update({
            'qwen_decoder_nums': 28,  # Teacher: μ „μ²΄ λ μ΄μ–΄
        })
        
        # teacher_model = LLM4RecQwen3Teacher(**teacher_args)
        # print("β… Teacher λ¨λΈ λ΅λ“ μ„±κ³µ")
        
        print("π“ Student λ¨λΈ λ΅λ“ ν…μ¤νΈ...")
        student_args = model_args.copy()
        student_args.update({
            'qwen_decoder_nums': 14,  # Student: μ λ° λ μ΄μ–΄
            'distill_block': 4,
            'is_cls_multiple': False,
        })
        
        # student_model = LLM4RecQwen3Student(**student_args)
        # print("β… Student λ¨λΈ λ΅λ“ μ„±κ³µ")
        
        print("π“‹ λ¨λΈ μ„¤μ • κ²€μ¦:")
        print(f"  - Base Model: {base_model}")
        print(f"  - Teacher Layers: {teacher_args['qwen_decoder_nums']}")
        print(f"  - Student Layers: {student_args['qwen_decoder_nums']}")
        print(f"  - LoRA Config: r={model_args['lora_r']}, alpha={model_args['lora_alpha']}")
        print(f"  - Target Modules: {model_args['lora_target_modules']}")
        
        print("β… λ¨λ“  νΈν™μ„± ν…μ¤νΈ ν†µκ³Ό!")
        return True
        
    except FileNotFoundError as e:
        print(f"β νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {e}")
        print("   π’΅ λ‹¤μμ„ ν™•μΈν•΄μ£Όμ„Έμ”:")
        print(f"   - ./output/{domain_type}.pkl νμΌ μ΅΄μ¬ μ—¬λ¶€")
        print(f"   - ./dataset/sequential/{domain_type}.csv νμΌ μ΅΄μ¬ μ—¬λ¶€")
        return False
        
    except Exception as e:
        print(f"β μμƒμΉ λ»ν• μ¤λ¥ λ°μƒ: {e}")
        print(f"   μ¤λ¥ νƒ€μ…: {type(e).__name__}")
        return False

def print_usage_guide():
    """μ‚¬μ©λ²• κ°€μ΄λ“ μ¶λ ¥"""
    print("\n" + "="*60)
    print("π€ Qwen3 Knowledge Distillation μ‚¬μ© κ°€μ΄λ“")
    print("="*60)
    print()
    print("1. κΈ°λ³Έ μ‹¤ν–‰:")
    print("   bash run_distill_qwen3.sh")
    print()
    print("2. μ»¤μ¤ν…€ μ„¤μ •μΌλ΅ μ‹¤ν–‰:")
    print("   python distill_qwen3.py \\")
    print("     --base_model 'Qwen/Qwen3-8B' \\")
    print("     --domain_type 'music' \\")
    print("     --qwen_decoder_nums_teacher 28 \\")
    print("     --qwen_decoder_nums_student 14 \\")
    print("     --distill_type_standard 'offline' \\")
    print("     --gpu_device 0")
    print()
    print("3. ν‰κ°€λ§ μ‹¤ν–‰:")
    print("   python distill_qwen3.py \\")
    print("     --train_eval_type 'eval' \\")
    print("     --student_resume_from_checkpoint 'path/to/checkpoint'")
    print()
    print("π’΅ μ£Όμ” μ°¨μ΄μ :")
    print("   - LLaMA β†’ Qwen3-8B λ¨λΈ μ‚¬μ©")
    print("   - λ‹¨μΌ GPU ν™κ²½μ— μµμ ν™”")
    print("   - bfloat16 μ •λ°€λ„ μ‚¬μ©")
    print("   - ν–¥μƒλ μ•μ •μ„±κ³Ό λ©”λ¨λ¦¬ ν¨μ¨μ„±")
    print()

if __name__ == "__main__":
    print("π”¥ Qwen3 Knowledge Distillation Framework")
    print("=" * 50)
    
    # νΈν™μ„± ν…μ¤νΈ μ‹¤ν–‰
    success = test_qwen3_models()
    
    if success:
        print_usage_guide()
    else:
        print("\nβ νΈν™μ„± ν…μ¤νΈ μ‹¤ν¨. μ„μ μ•λ‚΄λ¥Ό ν™•μΈν•κ³  λ‹¤μ‹ μ‹λ„ν•΄μ£Όμ„Έμ”.")
    
    print("\n" + "="*60)
